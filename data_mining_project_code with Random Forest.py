# -*- coding: utf-8 -*-
"""(merging) Data Mining project code - Albert - Random.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gisuxx9IgxXFG4GLi45VJBXXgLorD7yk
"""

import pandas as pd
# from google.colab import drive # Uncomment in case you are importing the dataset from Google Drive.
from sklearn.utils import resample
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
import seaborn as sns
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Importing dependencies

# Code to replace missing values using KNN and predicting the values

train_data = pd.read_csv("./census-income.data.csv")
test_data = pd.read_csv("./census-income.test.csv")

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Encode 'marital-status' column
train_data['marital-status'] = label_encoder.fit_transform(train_data['marital-status'])
train_data['relationship'] = label_encoder.fit_transform(train_data['relationship'])
train_data['race'] = label_encoder.fit_transform(train_data['race'])
train_data['sex'] = label_encoder.fit_transform(train_data['sex'])
## convert 'income' data to numerical values
train_data['income'] = label_encoder.fit_transform(train_data['income'])

## replace 'native-country' with mode
train_data['native-country'] = train_data['native-country'].replace('?', 'United-States')
label_encoder = LabelEncoder()
train_data['native-country'] = label_encoder.fit_transform(train_data['native-country'])

## replace 'work-class' with mode
train_data['work-class'] = train_data['work-class'].replace('?', 'Private')
train_data['work-class'] = label_encoder.fit_transform(train_data['work-class'])

x_train = train_data.drop(['education','income'], axis=1)
y_train = train_data["income"]

from sklearn.impute import KNNImputer
from sklearn.preprocessing import OneHotEncoder

# Replace '?' with NaN in the 'occupation' column
x_train['occupation'].replace('?', np.nan, inplace=True)

# Separate features (X) and target variable (y)
X_train = x_train.drop('occupation', axis=1)
y_occupation_train = x_train['occupation']

# Apply KNN Imputer
knn_imputer = KNNImputer()
X_imputed_train = knn_imputer.fit_transform(X_train)

# Convert imputed data back to DataFrame
X_imputed_df_train = pd.DataFrame(X_imputed_train, columns=X_train.columns)

print(X_imputed_df_train)

# Combine imputed data with original data
imputed_x_train = X_imputed_df_train.copy()
imputed_x_train['occupation'] = y_occupation_train
imputed_x_train['occupation'] = label_encoder.fit_transform(imputed_x_train['occupation'])

'''
In case, you want to run the code from Google Drive, follow these steps.
Otherwise, ignore the Google Drive commands.

Here, we authenticate with Google Drive to add the datafiles from a Drive folder

Instructions:

1. Create a folder in your Google Drive, called data-mining-csv-files
2. Insert both csv files (test and train) with the original names there.
3. Just run this cell to import the code
'''

# from google.colab import drive
# drive.mount('/content/drive')

# train_data = pd.read_csv("/content/drive/My Drive/data-mining-csv-files/census-income.data.csv") #- Albert: remove comment if you want to manually place the files here. If not, it's going to use a Google Drive folder
# test_data = pd.read_csv("/content/drive/My Drive/data-mining-csv-files/census-income.test.csv")  #- Albert: remove comment if you want to manually place the files here. If not, it's going to use a Google Drive folder

'''
Importing datasets.

These are the cleaned datasets without missing values,
which were replaced by KNN algorithm. The corresponding code is in
the previous blocks.
'''

train_data = pd.read_csv("./train_data_d.csv")
test_data = pd.read_csv("./test_data_d.csv")

# Displaying results:
print(train_data)
print(test_data)

# Class Distribution: Plot the distribution of the target variable (income)
# to check for class imbalance.

# Set up a subplot grid for training and testing distributions
fig, axs = plt.subplots(1, 2, figsize=(14, 5))

# Different sky-blue-themed colors
train_palette = ['skyblue', 'deepskyblue']
test_palette = ['skyblue', 'deepskyblue']

# Training data
sns.countplot(x='income', data=train_data, ax=axs[0], palette=train_palette)
axs[0].set_title('Training Data Income Distribution')
axs[0].set_ylabel('Count')
axs[0].set_xlabel('Income')

# Testing data
sns.countplot(x='income', data=test_data, ax=axs[1], palette=test_palette)
axs[1].set_title('Testing Data Income Distribution')
axs[1].set_ylabel('Count')
axs[1].set_xlabel('Income')

# Adjust layout
plt.tight_layout()
plt.show()

'''
Model 1: imbalanced dataset and One Hot Encoding
'''
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report


# Function to preprocess data
def preprocess_data(data, transformer=None, is_train=True):
    print("Initial columns:", data.columns)  # Initial check of all columns in the dataset

    income_column = 'income'
    if income_column not in data.columns:
        raise ValueError(f"The column '{income_column}' does not exist in the dataset.")

    # If it's test data, handle the period at the end
    if not is_train:
        # Convert values to strings before applying string methods
        data[income_column] = data[income_column].astype(str).str.strip().str.replace('.', '')
        print("Adjusted 'income' for test data:", data[income_column].unique())

    # Identify and store categorical features excluding 'income'
    categorical_features = data.select_dtypes(include=['object']).columns.tolist()
    if income_column in categorical_features:
        categorical_features.remove(income_column)
    print("Categorical features before encoding:", categorical_features)

    # Apply ColumnTransformer only to categorical features excluding 'income'
    if transformer is None:
        transformer = ColumnTransformer(
            transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
            remainder='passthrough')
        X = transformer.fit_transform(data.drop(columns=[income_column]))
    else:
        X = transformer.transform(data.drop(columns=[income_column]))

    # Now encode the 'income' column after categorical handling
    y = LabelEncoder().fit_transform(data[income_column])
    print("Encoded 'income':", np.unique(y))

    return X, y, transformer


# Function to train the Random Forest model
def train_random_forest(X, y):
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X, y)

    return model

# Evaluate the model
def evaluate_model(model, X, y):
    y_pred = model.predict(X)
    accuracy = accuracy_score(y, y_pred)
    report = classification_report(y, y_pred)

    return accuracy, report

# Main execution function
def main():
    # Preprocess training data
    X_train, y_train, transformer = preprocess_data(train_data, is_train=True)

    # Train model without SMOTE
    model = train_random_forest(X_train, y_train)

    # Preprocess testing data using the same transformer
    X_test, y_test, _ = preprocess_data(test_data, transformer, is_train=False)

    # Evaluate the test model
    accuracy_test, report_test = evaluate_model(model, X_test, y_test)
    print(f'Accuracy (Test Data): {accuracy_test}')
    print(f'Classification Report (Test Data):\n{report_test}')

    # Evaluate the training model to check for overfitting
    accuracy_train, report_train = evaluate_model(model, X_train, y_train)
    print(f'Accuracy (Training Data): {accuracy_train}')
    print(f'Classification Report (Training Data):\n{report_train}')

if __name__ == '__main__':
    main()

'''
Model 2: Balanced dataset (SMOTE) and One Hot Encoding
'''

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE


# Assuming train_data and test_data are already loaded as dataframes
# train_data, test_data

# Function to preprocess data

def preprocess_data(data, transformer=None, is_train=True):
    print("Initial columns:", data.columns)  # Initial check of all columns in the dataset

    income_column = 'income'
    if income_column not in data.columns:
        raise ValueError(f"The column '{income_column}' does not exist in the dataset.")

    # If it's test data, handle the period at the end
    if not is_train:
        data[income_column] = data[income_column].str.strip().str.replace('.', '')
        print("Adjusted 'income' for test data:", data[income_column].unique())  # Debug

    # Identify and store categorical features excluding 'income'
    categorical_features = data.select_dtypes(include=['object']).columns.tolist()
    if income_column in categorical_features:
        categorical_features.remove(income_column)
    print("Categorical features before encoding:", categorical_features)  # Debug statement

    # Apply ColumnTransformer only to categorical features excluding 'income'
    if transformer is None:
        transformer = ColumnTransformer(
            transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
            remainder='passthrough')
        X = transformer.fit_transform(data.drop(columns=[income_column]))
    else:
        X = transformer.transform(data.drop(columns=[income_column]))

    # Now encode the 'income' column after categorical handling
    y = LabelEncoder().fit_transform(data[income_column])
    print("Encoded 'income':", np.unique(y))  # Debug the output of the encoded income

    return X, y, transformer


# Function to train the Random Forest model with SMOTE
def train_random_forest_with_smote(X, y):
    # Apply SMOTE
    smote = SMOTE(random_state=42)
    X_smote, y_smote = smote.fit_resample(X, y)

    # Train the Random Forest
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_smote, y_smote)

    return model

# Evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    return accuracy, report

# Main execution function
def main():
    # Preprocess training data
    X_train, y_train, transformer = preprocess_data(train_data, is_train=True)

    # Train model with SMOTE
    smote = SMOTE(random_state=42)
    X_smote, y_smote = smote.fit_resample(X_train, y_train)
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_smote, y_smote)

    # Preprocess testing data using the same transformer
    X_test, y_test, _ = preprocess_data(test_data, transformer, is_train=False)

    # Evaluate the test model
    accuracy, report = evaluate_model(model, X_test, y_test)
    print(f'Accuracy test: {accuracy}')
    print(f'Classification Report test:\n{report}')

    # Evaluate the test model
    accuracy_train, report_train = evaluate_model(model, X_train, y_train)
    print(f'Accuracy train: {accuracy_train}')
    print(f'Classification Report train:\n{report_train}')

if __name__ == '__main__':
    main()

"""Initial columns: Index(['age', 'work-class', 'fnlwgt', 'education', 'education_num',
       'marital-status', 'occupation', 'relationship', 'race', 'sex',
       'capital-gain', 'capital-loss', 'hours-worked-per-week',
       'native-country', 'income'],
      dtype='object')
Categorical features before encoding: []
Encoded 'income': [0 1]
Initial columns: Index(['age', 'work-class', 'fnlwgt', 'education', 'education_num',
       'marital-status', 'occupation', 'relationship', 'race', 'sex',
       'capital-gain', 'capital-loss', 'hours-worked-per-week',
       'native-country', 'income'],
      dtype='object')
Adjusted 'income' for test data: ['<=50K' '>50K']
Categorical features before encoding: []
Encoded 'income': [0 1]
Accuracy test: 0.8431914501566243
Classification Report test:
              precision    recall  f1-score   support

           0       0.90      0.90      0.90     12435
           1       0.67      0.67      0.67      3846

    accuracy                           0.84     16281
   macro avg       0.78      0.78      0.78     16281
weighted avg       0.84      0.84      0.84     16281

Accuracy train: 0.9999385768250361
Classification Report train:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     24720
           1       1.00      1.00      1.00      7841

    accuracy                           1.00     32561
   macro avg       1.00      1.00      1.00     32561
weighted avg       1.00      1.00      1.00     32561
"""

'''
One Hot Encoding Implementation # removing fnlweight column, version 2
# Version 2

Outcome: less precision
'''

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE


# Assuming train_data and test_data are already loaded as dataframes
# train_data, test_data

# Function to preprocess data

def preprocess_data(data, transformer=None, is_train=True):
    print("Initial columns:", data.columns)  # Initial check of all columns in the dataset

    income_column = 'income'
    fnlwgt_column = 'fnlwgt'  # Define the column name to be dropped

    if income_column not in data.columns:
        raise ValueError(f"The column '{income_column}' does not exist in the dataset.")

    # Drop the 'fnlwgt' column from the dataset
    if fnlwgt_column in data.columns:
        data = data.drop(columns=[fnlwgt_column])
        print(f"'{fnlwgt_column}' column dropped.")
    else:
        print(f"No '{fnlwgt_column}' column to drop.")

    # If it's test data, handle the period at the end
    if not is_train:
        data[income_column] = data[income_column].str.strip().str.replace('.', '')
        print("Adjusted 'income' for test data:", data[income_column].unique())  # Debug

    # Identify and store categorical features excluding 'income'
    categorical_features = data.select_dtypes(include=['object']).columns.tolist()
    if income_column in categorical_features:
        categorical_features.remove(income_column)
    print("Categorical features before encoding:", categorical_features)  # Debug statement

    # Apply ColumnTransformer only to categorical features excluding 'income'
    if transformer is None:
        transformer = ColumnTransformer(
            transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
            remainder='passthrough')
        X = transformer.fit_transform(data.drop(columns=[income_column]))
    else:
        X = transformer.transform(data.drop(columns=[income_column]))

    # Now encode the 'income' column after categorical handling
    y = LabelEncoder().fit_transform(data[income_column])
    print("Encoded 'income':", np.unique(y))  # Debug the output of the encoded income

    return X, y, transformer

# Function to train the Random Forest model with SMOTE
def train_random_forest_with_smote(X, y):
    # Apply SMOTE
    smote = SMOTE(random_state=42)
    X_smote, y_smote = smote.fit_resample(X, y)

    # Train the Random Forest
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_smote, y_smote)

    return model

# Evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    return accuracy, report

# Main execution function
def main():
    # Preprocess training data
    X_train, y_train, transformer = preprocess_data(train_data, is_train=True)

    # Train model with SMOTE
    smote = SMOTE(random_state=42)
    X_smote, y_smote = smote.fit_resample(X_train, y_train)
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_smote, y_smote)

    # Preprocess testing data using the same transformer
    X_test, y_test, _ = preprocess_data(test_data, transformer, is_train=False)

    # Evaluate the model
    accuracy, report = evaluate_model(model, X_test, y_test)
    print(f'Accuracy: {accuracy}')
    print(f'Classification Report:\n{report}')

if __name__ == '__main__':
    main()

"""Initial columns: Index(['age', 'work-class', 'fnlwgt', 'education', 'education_num',
       'marital-status', 'occupation', 'relationship', 'race', 'sex',
       'capital-gain', 'capital-loss', 'hours-worked-per-week',
       'native-country', 'income'],
      dtype='object')
'fnlwgt' column dropped.
Categorical features before encoding: ['work-class', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']
Encoded 'income': [0 1]
Initial columns: Index(['age', 'work-class', 'fnlwgt', 'education', 'education_num',
       'marital-status', 'occupation', 'relationship', 'race', 'sex',
       'capital-gain', 'capital-loss', 'hours-worked-per-week',
       'native-country', 'income'],
      dtype='object')
'fnlwgt' column dropped.
Adjusted 'income' for test data: ['<=50K' '>50K']
Categorical features before encoding: ['work-class', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']
Encoded 'income': [0 1]
Accuracy: 0.839690436705362
Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.91      0.90     12435
           1       0.68      0.62      0.65      3846

    accuracy                           0.84     16281
   macro avg       0.78      0.76      0.77     16281
weighted avg       0.84      0.84      0.84     16281

"""

train_data.columns

'''
One Hot Encoding Implementation # default, version 3
Using Label Encoding, Smote
'''
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE

# Function to preprocess data using label encoding
def preprocess_data(data, encoders=None, is_train=True):
    print("Initial columns:", data.columns)  # Initial check of all columns in the dataset

    income_column = 'income'
    if income_column not in data.columns:
        raise ValueError(f"The column '{income_column}' does not exist in the dataset.")

    # If it's test data, handle the period at the end
    if not is_train:
        data[income_column] = data[income_column].str.strip().str.replace('.', '')
        print("Adjusted 'income' for test data:", data[income_column].unique())  # Debug

    # Identify categorical features excluding 'income'
    categorical_features = data.select_dtypes(include=['object']).columns.tolist()
    if income_column in categorical_features:
        categorical_features.remove(income_column)

    # Initialize or apply existing label encoders
    if encoders is None:
        encoders = {}
        for feature in categorical_features:
            le = LabelEncoder()
            data[feature] = le.fit_transform(data[feature])
            encoders[feature] = le
    else:
        for feature, le in encoders.items():
            data[feature] = le.transform(data[feature])

    print("Encoded features:", categorical_features)  # Debug statement

    # Encode the target variable
    y = LabelEncoder().fit_transform(data[income_column])
    print("Encoded 'income':", np.unique(y))  # Debug the output of the encoded income

    # All features are now numeric, so return the numeric dataframe directly
    X = data.drop(columns=[income_column]).values

    return X, y, encoders

# Function to train the Random Forest model with SMOTE
def train_random_forest_with_smote(X, y):
    # Apply SMOTE
    smote = SMOTE(random_state=42)
    X_smote, y_smote = smote.fit_resample(X, y)

    # Train the Random Forest
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_smote, y_smote)

    return model

# Evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    return accuracy, report

# Main execution function
def main():
    # Preprocess training data with Label Encoding
    X_train, y_train, encoders = preprocess_data(train_data, is_train=True)

    # Train model with SMOTE
    smote = SMOTE(random_state=42)
    X_smote, y_smote = smote.fit_resample(X_train, y_train)
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_smote, y_smote)

    # Preprocess testing data using the same encoders
    X_test, y_test, _ = preprocess_data(test_data, encoders, is_train=False)

    # Evaluate the model
    accuracy, report = evaluate_model(model, X_test, y_test)
    print(f'Accuracy: {accuracy}')
    print(f'Classification Report:\n{report}')

    # Evaluate the model
    accuracy1, report1 = evaluate_model(model, X_train, y_train)
    print(f'Accuracy: {accuracy1}')
    print(f'Classification Report:\n{report1}')

if __name__ == '__main__':
    main()

"""Initial columns: Index(['age', 'work-class', 'fnlwgt', 'education', 'education_num',
       'marital-status', 'occupation', 'relationship', 'race', 'sex',
       'capital-gain', 'capital-loss', 'hours-worked-per-week',
       'native-country', 'income'],
      dtype='object')
Encoded features: []
Encoded 'income': [0 1]
Initial columns: Index(['age', 'work-class', 'fnlwgt', 'education', 'education_num',
       'marital-status', 'occupation', 'relationship', 'race', 'sex',
       'capital-gain', 'capital-loss', 'hours-worked-per-week',
       'native-country', 'income'],
      dtype='object')
Adjusted 'income' for test data: ['<=50K' '>50K']
Encoded features: []
Encoded 'income': [0 1]
Accuracy: 0.8431914501566243
Classification Report:
              precision    recall  f1-score   support

           0       0.90      0.90      0.90     12435
           1       0.67      0.67      0.67      3846

    accuracy                           0.84     16281
   macro avg       0.78      0.78      0.78     16281
weighted avg       0.84      0.84      0.84     16281

Accuracy: 0.9999385768250361
Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     24720
           1       1.00      1.00      1.00      7841

    accuracy                           1.00     32561
   macro avg       1.00      1.00      1.00     32561
weighted avg       1.00      1.00      1.00     32561
"""

'''
# version 4
Using Label Encoding, Without Smote
'''
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Function to preprocess data using label encoding
def preprocess_data(data, encoders=None, is_train=True):
    print("Initial columns:", data.columns)  # Initial check of all columns in the dataset

    income_column = 'income'
    if income_column not in data.columns:
        raise ValueError(f"The column '{income_column}' does not exist in the dataset.")

    # If it's test data, handle the period at the end
    if not is_train:
        data[income_column] = data[income_column].str.strip().str.replace('.', '')
        print("Adjusted 'income' for test data:", data[income_column].unique())  # Debug

    # Identify categorical features excluding 'income'
    categorical_features = data.select_dtypes(include=['object']).columns.tolist()
    if income_column in categorical_features:
        categorical_features.remove(income_column)

    # Initialize or apply existing label encoders
    if encoders is None:
        encoders = {}
        for feature in categorical_features:
            le = LabelEncoder()
            data[feature] = le.fit_transform(data[feature])
            encoders[feature] = le
    else:
        for feature, le in encoders.items():
            data[feature] = le.transform(data[feature])

    print("Encoded features:", categorical_features)  # Debug statement

    # Encode the target variable
    y = LabelEncoder().fit_transform(data[income_column])
    print("Encoded 'income':", np.unique(y))  # Debug the output of the encoded income

    # All features are now numeric, so return the numeric dataframe directly
    X = data.drop(columns=[income_column]).values

    return X, y, encoders

# Function to train the Random Forest model without SMOTE
def train_random_forest(X, y):
    # Train the Random Forest without SMOTE
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X, y)

    return model

# Evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    return accuracy, report

# Main execution function
def main():
    # Preprocess training data with Label Encoding
    X_train, y_train, encoders = preprocess_data(train_data, is_train=True)

    # Train the Random Forest without SMOTE
    model = train_random_forest(X_train, y_train)

    # Preprocess testing data using the same encoders
    X_test, y_test, _ = preprocess_data(test_data, encoders, is_train=False)

    # Evaluate the model
    accuracy, report = evaluate_model(model, X_test, y_test)
    print(f'Accuracy: {accuracy}')
    print(f'Classification Report:\n{report}')

if __name__ == '__main__':
    main()

"""Initial columns: Index(['age', 'work-class', 'fnlwgt', 'education', 'education_num',
       'marital-status', 'occupation', 'relationship', 'race', 'sex',
       'capital-gain', 'capital-loss', 'hours-worked-per-week',
       'native-country', 'income'],
      dtype='object')
Encoded features: []
Encoded 'income': [0 1]
Initial columns: Index(['age', 'work-class', 'fnlwgt', 'education', 'education_num',
       'marital-status', 'occupation', 'relationship', 'race', 'sex',
       'capital-gain', 'capital-loss', 'hours-worked-per-week',
       'native-country', 'income'],
      dtype='object')
Adjusted 'income' for test data: ['<=50K' '>50K']
Encoded features: []
Encoded 'income': [0 1]
Accuracy: 0.8534488053559364
Classification Report:
              precision    recall  f1-score   support

           0       0.88      0.93      0.91     12435
           1       0.73      0.61      0.66      3846

    accuracy                           0.85     16281
   macro avg       0.81      0.77      0.78     16281
weighted avg       0.85      0.85      0.85     16281

"""

'''
 # version 5
Using Label Encoding, Without Smote
Using hyperparameter tuning (gridsearchcv)
'''

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV

# Function to preprocess data using label encoding
def preprocess_data(data, encoders=None, is_train=True):
    print("Initial columns:", data.columns)  # Initial check of all columns in the dataset

    income_column = 'income'
    if income_column not in data.columns:
        raise ValueError(f"The column '{income_column}' does not exist in the dataset.")

    # If it's test data, handle the period at the end
    if not is_train:
        data[income_column] = data[income_column].str.strip().str.replace('.', '')
        print("Adjusted 'income' for test data:", data[income_column].unique())  # Debug

    # Identify categorical features excluding 'income'
    categorical_features = data.select_dtypes(include=['object']).columns.tolist()
    if income_column in categorical_features:
        categorical_features.remove(income_column)

    # Initialize or apply existing label encoders
    if encoders is None:
        encoders = {}
        for feature in categorical_features:
            le = LabelEncoder()
            data[feature] = le.fit_transform(data[feature])
            encoders[feature] = le
    else:
        for feature, le in encoders.items():
            data[feature] = le.transform(data[feature])

    print("Encoded features:", categorical_features)  # Debug statement

    # Encode the target variable
    y = LabelEncoder().fit_transform(data[income_column])
    print("Encoded 'income':", np.unique(y))  # Debug the output of the encoded income

    # All features are now numeric, so return the numeric dataframe directly
    X = data.drop(columns=[income_column]).values

    return X, y, encoders

# Function to perform hyperparameter tuning with RandomForestClassifier
def train_random_forest_with_tuning(X, y):
    # Define parameter grid for hyperparameter tuning
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 20, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    # Initialize RandomForestClassifier
    rf = RandomForestClassifier(random_state=42)

    # Use GridSearchCV for hyperparameter tuning
    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')
    grid_search.fit(X, y)

    # Return the best model
    print(f"Best Parameters: {grid_search.best_params_}")
    return grid_search.best_estimator_

# Evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    return accuracy, report

# Main execution function
def main():
    # Preprocess training data with Label Encoding
    X_train, y_train, encoders = preprocess_data(train_data, is_train=True)

    # Train the Random Forest with hyperparameter tuning
    model = train_random_forest_with_tuning(X_train, y_train)

    # Preprocess testing data using the same encoders
    X_test, y_test, _ = preprocess_data(test_data, encoders, is_train=False)

    # Evaluate the model
    accuracy, report = evaluate_model(model, X_test, y_test)
    print(f'Accuracy: {accuracy}')
    print(f'Classification Report:\n{report}')

if __name__ == '__main__':
    main()

'''
Version 7: no smote, OHE, using parameters
'''

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report


# Assuming train_data and test_data are already loaded as dataframes
# train_data, test_data

# Function to preprocess data
def preprocess_data(data, transformer=None, is_train=True):
    print("Initial columns:", data.columns)  # Initial check of all columns in the dataset

    income_column = 'income'
    if income_column not in data.columns:
        raise ValueError(f"The column '{income_column}' does not exist in the dataset.")

    # If it's test data, handle the period at the end
    if not is_train:
        data[income_column] = data[income_column].str.strip().str.replace('.', '')
        print("Adjusted 'income' for test data:", data[income_column].unique())  # Debug

    # Identify and store categorical features excluding 'income'
    categorical_features = data.select_dtypes(include=['object']).columns.tolist()
    if income_column in categorical_features:
        categorical_features.remove(income_column)
    print("Categorical features before encoding:", categorical_features)  # Debug statement

    # Apply ColumnTransformer only to categorical features excluding 'income'
    if transformer is None:
        transformer = ColumnTransformer(
            transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
            remainder='passthrough')
        X = transformer.fit_transform(data.drop(columns=[income_column]))
    else:
        X = transformer.transform(data.drop(columns=[income_column]))

    # Now encode the 'income' column after categorical handling
    y = LabelEncoder().fit_transform(data[income_column])
    print("Encoded 'income':", np.unique(y))  # Debug the output of the encoded income

    return X, y, transformer

# Function to train the Random Forest model with specified hyperparameters
def train_random_forest(X, y):
    # Initialize the RandomForestClassifier with the desired hyperparameters
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_leaf=1,
        min_samples_split=2,
        bootstrap=True,
        random_state=42
    )
    model.fit(X, y)

    return model

# Evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    return accuracy, report

# Main execution function
def main():
    # Preprocess training data
    X_train, y_train, transformer = preprocess_data(train_data, is_train=True)

    # Train model using specified hyperparameters
    model = train_random_forest(X_train, y_train)

    # Preprocess testing data using the same transformer
    X_test, y_test, _ = preprocess_data(test_data, transformer, is_train=False)

    # Evaluate the model on the test data
    accuracy, report = evaluate_model(model, X_test, y_test)
    print(f'Accuracy (Test Data): {accuracy}')
    print(f'Classification Report (Test Data):\n{report}')

    # Evaluate the model on the training data
    accuracy_train, report_train = evaluate_model(model, X_train, y_train)
    print(f'Accuracy (Training Data): {accuracy_train}')
    print(f'Classification Report (Training Data):\n{report_train}')

if __name__ == '__main__':
    # Load your datasets (replace with actual file paths)
    # train_data = pd.read_csv('./census-income.data (3).csv')
    # test_data = pd.read_csv('./census-income.test (2).csv')

    main()

'''
Model 3: Balanced dataset (Smote), O.H.E. and Hyperparameter
tuning (focused on accuracy), in the balanced dataset
Using: ACCURACY focused prediction
'''

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE


def preprocess_data(data, transformer=None, is_train=True):
    print("Initial columns:", data.columns)

    income_column = 'income'
    if income_column not in data.columns:
        raise ValueError(f"The column '{income_column}' does not exist in the dataset.")

    # Convert 'income' to string explicitly to avoid AttributeError
    data[income_column] = data[income_column].astype(str)

    # If it's test data, handle the period at the end
    if not is_train:
        data[income_column] = data[income_column].str.strip().str.replace('.', '', regex=False)
        print("Adjusted 'income' for test data:", data[income_column].unique())

    # Identify and store categorical features excluding 'income'
    categorical_features = data.select_dtypes(include=['object']).columns.tolist()
    if income_column in categorical_features:
        categorical_features.remove(income_column)
    print("Categorical features before encoding:", categorical_features)

    # Apply ColumnTransformer only to categorical features excluding 'income'
    if transformer is None:
        transformer = ColumnTransformer(
            transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
            remainder='passthrough')
        X = transformer.fit_transform(data.drop(columns=[income_column]))
    else:
        X = transformer.transform(data.drop(columns=[income_column]))

    # Now encode the 'income' column after categorical handling
    y = LabelEncoder().fit_transform(data[income_column])
    print("Encoded 'income':", np.unique(y))  # Debug the output of the encoded income

    return X, y, transformer

# Function to train the Random Forest model with SMOTE
def train_random_forest_with_smote(X, y):
    # Apply SMOTE
    smote = SMOTE(random_state=42)
    X_smote, y_smote = smote.fit_resample(X, y)

    # Define the Random Forest model with specified hyperparameters
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=20,
        min_samples_leaf=1,
        min_samples_split=10,
        bootstrap=False,
        random_state=42
    )

    # Train (fit) the model using the resampled data
    model.fit(X_smote, y_smote)
    return model

# Function to evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    return accuracy, report

# Main execution function
def main():
    # Preprocess training data
    X_train, y_train, transformer = preprocess_data(train_data, is_train=True)

    # Train model with SMOTE and specified hyperparameters
    model = train_random_forest_with_smote(X_train, y_train)

    # Preprocess testing data using the same transformer
    X_test, y_test, _ = preprocess_data(test_data, transformer, is_train=False)

    # Evaluate the test model
    accuracy, report = evaluate_model(model, X_test, y_test)
    print(f'Accuracy test: {accuracy}')
    print(f'Classification Report test:\n{report}')

    # Evaluate the training model
    accuracy_train, report_train = evaluate_model(model, X_train, y_train)
    print(f'Accuracy train: {accuracy_train}')
    print(f'Classification Report train:\n{report_train}')

if __name__ == '__main__':
    main()

'''
Model 4: Balanced dataset (SMOTE), O.H.E. and Hyperparameter
tuning (focused on a balance), in the balanced dataset
Using: BALANCED focused prediction
'''

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from imblearn.over_sampling import SMOTE

# Preprocessing data
def preprocess_data(data, transformer=None, is_train=True):
    print("Initial columns:", data.columns)

    income_column = 'income'
    if income_column not in data.columns:
        raise ValueError(f"The column '{income_column}' does not exist in the dataset.")

    # Convert 'income' to string explicitly to avoid AttributeError
    data[income_column] = data[income_column].astype(str)

    # If it's test data, handle the period at the end
    if not is_train:
        data[income_column] = data[income_column].str.strip().str.replace('.', '', regex=False)
        print("Adjusted 'income' for test data:", data[income_column].unique())

    # Identify and store categorical features excluding 'income'
    categorical_features = data.select_dtypes(include=['object']).columns.tolist()
    if income_column in categorical_features:
        categorical_features.remove(income_column)
    print("Categorical features before encoding:", categorical_features)

    # Apply ColumnTransformer only to categorical features excluding 'income'
    if transformer is None:
        transformer = ColumnTransformer(
            transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
            remainder='passthrough')
        X = transformer.fit_transform(data.drop(columns=[income_column]))
    else:
        X = transformer.transform(data.drop(columns=[income_column]))

    # Now encode the 'income' column after categorical handling
    y = LabelEncoder().fit_transform(data[income_column])
    print("Encoded 'income':", np.unique(y))

    return X, y, transformer

# Function to train the Random Forest model with SMOTE
def train_random_forest_with_smote(X, y):
    # Apply SMOTE
    smote = SMOTE(random_state=42)
    X_smote, y_smote = smote.fit_resample(X, y)

    # Define the Random Forest model with specified hyperparameters
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_leaf=1,
        min_samples_split=2,
        bootstrap=False,
        random_state=42
    )

    # Train (fit) the model using the resampled data
    model.fit(X_smote, y_smote)
    return model

# Function to evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)

    return accuracy, report

# Main execution function
def main():
    # Preprocess training data
    X_train, y_train, transformer = preprocess_data(train_data, is_train=True)

    # Train model with SMOTE and specified hyperparameters
    model = train_random_forest_with_smote(X_train, y_train)

    # Preprocess testing data using the same transformer
    X_test, y_test, _ = preprocess_data(test_data, transformer, is_train=False)

    # Evaluate the test model
    accuracy, report = evaluate_model(model, X_test, y_test)
    print(f'Accuracy test: {accuracy}')
    print(f'Classification Report test:\n{report}')

    # Evaluate the training model
    accuracy_train, report_train = evaluate_model(model, X_train, y_train)
    print(f'Accuracy train: {accuracy_train}')
    print(f'Classification Report train:\n{report_train}')

if __name__ == '__main__':
    main()



'''
Code block to replace missing values with mode
'''

# This function takes a dataframe and returns * A MODIFIED COPY *. Doesn't change the original
def replace_question_marks(df, columns_to_replace):
    most_common_elements = {}
    most_common_counts = {}
    df_copy = df.copy()  # Create a copy of the original DataFrame

    for col in columns_to_replace:
        # Find the most common element and its count in the column
        value_counts = df[col].value_counts()
        most_common = value_counts.idxmax()
        most_common_count = value_counts.max()

        most_common_elements[col] = most_common
        most_common_counts[col] = most_common_count

        # Replace ' ?' with the most common element in the column
        df_copy[col] = df_copy[col].replace(' ?', most_common)

    return df_copy, most_common_elements, most_common_counts

columns_to_replace = ['work-class', 'occupation', 'native-country']